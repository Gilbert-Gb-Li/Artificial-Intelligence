## 笔记与问题

#### 决策树
- 原理
    - 模型训练，生成树
    - PPT
- 优缺点
    - 不需要做onehot，决策树是对标签做计算
    - 可解释性强
    - 对数据量大的时候会差一点
- 特点
    - 以特征为切分点，平行于轴切分，所以若数据沿轴方向分布则适合
    - 回归树时，计算的是标签的均值，所以做预测时无法如线性模型那样均匀输出？

#### 集成学习
- 容易过拟合的分类器：高方差低偏差的分类器 
- 容易欠拟合的分类器：低方差高偏差的分类器 
- 优化思路1：降低过拟合分类器的方差-
  - Bagging 类
  - 将一系列容易过拟合的分类器取平均 
  - 例子：随机森林
  - 例子：DropOut 
- 优化思路2：降低欠拟合分类器的偏差 
  - Boosting 类 
  - 将一系列容易欠拟合的分类器取加权平均 
  - 例子：广度神经网络 
  - 例子：Adaboost    
    [参考文章](https://www.cnblogs.com/further-further-further/p/9642899.html)

#### Adaboost
- [参考文档](https://www.cnblogs.com/pinard/p/6133937.html)     
![流程图](adaboost.png)
- 分类算法过程
    - 输入：样本集`$
        T={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}$`，标签为{-1,+1}，弱分类器算法,弱分类器迭代次数K。
    - 输出为最终的强分类器f(x)  
    1. 初始化样本集权重为
    ```math
        D(1)=(w_{11},w_{12},...w_{1m});\\
        w_{1i}=\frac1m;i=1,2...m
    ```
    2. 对于k=1,2，...K个分类器:
        1. 使用具有权重`$D_k$`的样本集来训练数据，得到弱分类器`$G_k(x)$`
        2. 计算`$G_k(x)$`的分类误差率
        ```math
            e_k=P(G_k(x_i)≠y_i)\\
            =∑_{i=1}^mw_{k,i}I(G_k(x_i)≠y_i)
        ```
        3. 计算弱分类器的系数
        ```math
        α_k=\frac12log(\frac{(1−e_k)}{e_k})
        ```
        4. 更新样本集的权重分布
        ```math
        w_{k+1,i}=\frac{w_{k,i}exp^{(−α_ky_iG_k(x_i))}}{Z_K}i=1,2,...m
        ```
        - 这里Zk是规范化因子
        ```math
        Z_k=∑_{i=1}^mw_{k,i}exp^{(−α_ky_iG_k(x_i))}
        ```
    3. 构建最终分类器为：
        ```math
        f(x)=sign(∑_{k=1}^Kα_kG_k(x))
        ```
    - 对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如AdaboostSAMME算法，它的弱分类器的系数
        ```math
        α_k=1/2log\frac{(1−e_k)}{e_k}+log(R−1)
        ```
        - 其中R为类别数。从上式可以看出，如果是二元分类，R=2，则上式和我们的二元分类算法中的弱分类器的系数一致。
- 回归算法过程
    - 与分类思路一致，仅参数计算方式不同
    1. 初始化样本集权重为
    ```math
        D(1)=(w_{11},w_{12},...w{1m});\\
        w_{1i}=1/m;i=1,2...m
    ```
    2. 对于k=1,2，...K个回归器:
        1. 使用具有权重`$D_k$`的样本集来训练数据，得到弱回归器`$G_k(x)$`
        2. 计算`$G_k(x)$`的回归误差率
            ```math
            e_{k,i}=\sum_{i=1}^mw_{k,i}e_{k,i}
            ```
            - 线性误差
            ```math
            e_{k,i}=\frac{|y_i−G_k(x_i)|}{E_k}
            ```
            - 均方误差
            ```math
            e_{k,i}=\frac{(y_i−G_k(x_i))^2}{E_k^2}
            ```
            - 指数误差
            ```math
            e_{k,i}=1-exp^{(\frac{|y_i−G_k(x_i)|}{E_k})}
            ```
            - 最大误差 
            ```math
                E_k=max|y_i−G_k(x_i)| i=1,2...m
            ```
        3. 计算弱分类器的系数
        ```math
        α_k=\frac{e_k}{1-e_k}
        ```
        4. 更新样本集的权重分布
        ```math
        w_{k+1,i}=\frac{w_{k,i}α_k^{1-e_{k,i}}}{Z_K}i=1,2,...m
        ```
        - 这里Zk是规范化因子
        ```math
        Z_k=∑_{i=1}^mw_{k,i}α_k^{1-e_{k,i}}
        ```
    3. 构建最终分类器为：
        ```math
        f(x)=G_k^*(x)
        ```
        - 与分类不同，取强回归器中权重中位数的弱回归器做回归
- 关于损失函数
    - adboost中误差、权重等公式皆是由损失函数推导而来
    - [参考文档](https://zhuanlan.zhihu.com/p/37358517)
    - 损失函数介绍
        - adaboost模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题
        ```math
        f_k(x)=f_{k-1}(x)+α_kG_k(x)
        ```
        - 损失函数
            ```math
            argmin_{α,G}\sum_{i=1}^mexp(-y_if_k(x_i))\\
            =argmin_{α,G}\sum_{i=1}^mexp^{[-y_i(f_{k-1}(x)+G_k(x_i))]}\\
            =argmin_{α,G}\sum_{i=1}^mexp^{-y_i(f_{k-1}(x)}\sum_{i=1}^mexp^{G_k(x_i))}
            ```
            - 令`$w_i^k=exp^{-y_if_{k-1}(x_i)}$`，不依赖与a与G，只是由`$f_{k-1}(x_i)$`计算出的定值，可以看作权重，所以
                ```math
                argmin_{α,G}\sum_{i=1}^mexp(-y_if_k(x_i))\\
                =\sum_{i=1}^mw_i^ke^{-y_iα_kG_k(x)}\\
                =e^-a\sum_{y=G_k}w_i^k+e^a\sum_{y\ne G_k}w_i^k
                ```
                - 当y=Gk时，`$I(y_i\ne G_k)$`为0，则`$e^{-y_iG_k(x)α_k}$`等于`$α_k$`，反之为`$-α_k$`
                - 以上对α求导，两边同乘以`$e_a$`，此处`$e_k$`为k分类器的误差
                    ```math
                    a_k=\frac12ln\frac{1-e_k}{e_k}
                    ```
- 优点
    - 可以灵活处理各种类型的数据、离散值和缺失值
    - 泛化错误率低，无需参数调整
    - 
- 缺点
    - 训练时间过长，容易受到噪声干扰
    - 执行效果依赖于弱分类器的选择
#### GBDT
- 拟合残差
- 回归当不使用均方误差，难以计算时，使用负梯度来近似残差
- 分类问题转化为概率来计算
- 参考【GBDT原理梳理】
- 优点
    - 适合低维数据
    - 处理各种类型的数据，包括连续值和离散值。
    - 对少的调参时间情况下，预测的准备率也可以比较高
    - 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数
- 缺点
    - 采用决策树，缺乏平滑性（回归预测时输出值只能是输出有限的若干种数值）
    - 不适合处理高维稀疏数据
    - 由于弱学习器之间存在较强依赖关系，难以并行训练
### 课程时间段
- part 1 [9:20 ~ 10:10]
    - 数据降维，提高准确度方法
    - 信息增益 ID3 算法 
- part 2 [10:35 ~ 11:40]
    - gini [10:43]
- part 3 [14:00 ~ 14:45]
    - 随机森林
- part 4 [15:00 ~ 15:40]
    - boost
- part 5 [15:55 ~ ]
    - GBDT 应用场景
    - Xgboost [20]


## 问题
- boost 预测时可以并行？