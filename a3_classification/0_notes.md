## 笔记与问题

## 知识点
- 约束最优化问题 ***** 
- 高斯分布是最大熵分布，在已知数据均值和方差的前提下

## SVM
- 原理    
**只考虑不容易区分的点并设定较高的权值**    
<font color=blue>参见代码[svm/svm.ipynb]</font>
- 核心：线性、RBF、多项式核心 ***
- 优点
    - 二分类比较精确
    - 相比于线性回归自带正则化，即只考虑难分点
    - 预测速度比较快
- 缺点
    - 当特征太多时，运算效率比较低且效果不太好
    - 当使用非线性核心或特征组合比较复杂时可能会过拟合

## Knn 
- 优缺点：
    - 简单容易，无需训练
    - 可实现非线性分类
    - 适合稀有数据分类
    - **特别适合多分类**
    - 缺点
        - 计算量大，内存开销大
        - 预测数据时间太长
- K，一般取20以内的整数
 
<br>

## 贝叶斯算法
- 特点
    - 贝叶斯为线性分类器，小规模样本上表现很好
    - 使用先验概率与条件概率来计算分类
    - 概率计算需连乘所以会存在数值问题，需要转为log计算
    - 特征值绝对大小对该算法影响不大
    - 假设过多，很多数据并不符合假设 
- 应用场景
    - 适合处理稀疏矩阵
    - 当特征间的线性相关性比较强时不适合该算法，例如图像

#### 1. 朴素贝叶斯
- 假设样本属性间是独立的，称为"朴素" 
- 数学原理：    
    ```
    p(y|x)p(x)=p(x|y)p(y)
    
    p(y|X)=\frac{p(y)*p(X|y)}{p(x)}
    
    =\frac{p(y)*p(X|y)}{\sum_c p(x;c)}
    ```
    - X为向量，计算复杂；所以假设X中的每个分量都是独立的

#### 2. 高斯环境下的朴素贝叶斯分类器
- 假设
    - 朴素
    - 每一类的每个特征都服从高斯分布
- 原理    
<font color=blue>参见代码[bayes/GNB.ipynb]</font>
- 说明
    - 在协方差相等的时候相当于线性分类器
    - 高斯朴素贝叶斯相当于在原特征的基础上条件X^2项，但效果达不到该程度
    - 该算法可将特征添加2次方项，而后用LR模型替代，且效果更好。
#### 3. 文本贝叶斯
- 文本向量化
    - 词频统计    
      参考代码[Feature/01.count-vect-base.py]
    - TF-IDF，词频加权算法    
    idf: log(总文档/含该词文档 + 1)，当某个词在所用文章都出现过认为不重要，如 '的'    
    [参考文档](https://zhuanlan.zhihu.com/p/31197209)
- 中文文本分类步骤
    1. 分词
    2. 向量化 
    3. 朴素贝叶斯做分类 

<br>

## 课程内容
- part 1 
    - 作业
    - 矩阵求导
    - 约束最优化问题
    - 高斯分布熵，泛函分析
    - svm原理1
- part 2 [ <<< ]
    - svm原理2 
    - 最优函数？
    - 核函数
        - 线性，假设数据线性可分
        - rbf，数据是非线性的
    - 代码展示
- part 3
    - 过拟合与欠拟合
    
=======================
- part 4
    - 贝叶斯算法 [<<<]
    - 高斯朴素贝叶斯实现
    - 文本向量化
- part 5
    - 文本分类及示例
        - 伯努利 与 多项式 模型
- part 6
    - 文本示例
    - 集成学习


## 问题
